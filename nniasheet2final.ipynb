{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise Sheet 2: Linear Algebra and TensorFlow Basics (Deadline: 14 Nov 14:00)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Linear Algebra (11 points)\n",
    "For theoretical tasks you are encouraged to write LaTeX. Jupyter notebooks support them simply by typing an expression between dollar signs or in the blocks like \\ begin{equation} \\ end{equation}.\n",
    "\n",
    "\n",
    "Alternatively, you can upload the solutions in the written form as images and paste them inside cells. But if you do this, make sure that the images have high quality, so we can read them without any problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Orthogonal vectors (1 point)\n",
    "Let $a, b$ be non-zero vectors in $R^n$. Show that $a^T b$ = 0 **if and only if** a and b are orthogonal vectors (i.e. angle between them equals $\\frac{\\pi}{2} + k \\pi$ radians).\n",
    "\n",
    "Hint: be careful to prove 2 directions: \n",
    "1.) $a^T b$ = 0 $\\implies$ orthogonality and 2.) orthogonality $\\implies$ $a^T b$ = 0.\n",
    "\n",
    "\n",
    "\n",
    "#### Direction 1: $a^T b$ = 0 $\\implies$ orthogonality\n",
    "\n",
    "$a^T b$ = 0\n",
    "\n",
    "$\\lVert \\mathbf{x} \\rVert_2 \\lVert \\mathbf{y} \\rVert_2 cos \\theta = 0 $ (Law of Cosines, see also Goodfellow 2.34)\n",
    "\n",
    "$cos \\theta = \\dfrac{0}{\\lVert \\mathbf{x} \\rVert_2 \\lVert \\mathbf{y} \\rVert_2} $\n",
    "\n",
    "$cos \\theta = 0 $\n",
    "\n",
    "$\\theta = \\frac{\\pi}{2} $\n",
    "\n",
    "a and b are thus orthogonal.\n",
    "\n",
    "\n",
    "#### Direction2: Orthogonality $\\implies$ $a^T b$ = 0.\n",
    "\n",
    "$a^T b = \\lVert \\mathbf{x} \\rVert_2 \\lVert \\mathbf{y} \\rVert_2 cos \\frac{\\pi}{2} $ (Law of Cosines, see also Goodfellow 2.34)\n",
    "\n",
    "$cos \\frac{\\pi}{2} = \\frac{a^T b}{\\lVert \\mathbf{x} \\rVert_2 \\lVert \\mathbf{y} \\rVert_2}$\n",
    "\n",
    "$0 = \\frac{a^T b}{\\lVert \\mathbf{x} \\rVert_2 \\lVert \\mathbf{y} \\rVert_2}$\n",
    "\n",
    "$a^T b = 0$\n",
    "\n",
    "### 2. Interpretation of matrix multiplication (2 points)\n",
    "Let \n",
    "$$\n",
    "A=\\begin{bmatrix} 2 & 4 \\\\ 4 & 2 \\\\ 2 & 0 \\end{bmatrix} \\ \\ \\ \\ \n",
    "B=\\begin{bmatrix} 0.6 & -0.8 \\\\ 0.8 & 0.6 \\end{bmatrix}\n",
    "$$\n",
    "a. Are the column vectors of B orthogonal? Orthonormal? Is B a rotation matrix? Justify your answers.(0.5 points)\n",
    "\n",
    "The column vectors are:\n",
    "\n",
    "Orthogonal because: $\\begin{bmatrix}0.6 \\\\ -0.8 \\end{bmatrix} \\cdot \\begin{bmatrix}0.8 \\\\ 0.6 \\end{bmatrix}$ = 0\n",
    "\n",
    "Orthonormal because: It is orthogonal and a Unit vector: $ \\sqrt{(0.6)^2 + (0.8)^2} = 1 $ and $\\sqrt{(-0.8)^2 + (0.6)^2} = 1 $\n",
    "\n",
    "B is a rotation matrix because $ B^T = B^{-1} => \\begin{bmatrix} 0.6 & 0.8 \\\\ -0.8 & 0.6 \\end{bmatrix} = 1* \\begin{bmatrix} 0.6 & 0.8 \\\\ -0.8 & 0.6 \\end{bmatrix}$ with determinant being 1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "b. Calculate $A \\cdot B$. (0.5 points)\n",
    "\n",
    "$A \\cdot B = \\begin{bmatrix} 4.4 & 0.8 \\\\ 4 & -2 \\\\ 1.2 & -1.6 \\end{bmatrix}$\n",
    "\n",
    "\n",
    "c. What is the interpretation of a matrix multiplication, when the column vectors of B are orthonormal (you may want to draw a simple picture visualizing row vectors of A and column vectors of B, but you don't need to submit it)? (1 point)\n",
    "\n",
    "In the given example, B is an orthogonal matrix. As a consequence, $B^T B = B B^T = I$. Multiplying an orthogonal matrix by a vector, does not change its length or angles. --> No idea what they are after here or what it means for matrix times matrix multiplication. Was mir auffällt: Euclidian norm rows in A und rows in C ist identisch (Wurzel 20, Wurzel 20, 4). Was heißt das dann? Länge bleibt.\n",
    "\n",
    "\n",
    "### 3. Property of $X^T X$ matrix (1 point)\n",
    "Let $X \\in R^{m×n}$ be a matrix obtained by stacking all training vectors (also called \"design matrix\"), like in the lecture. Obviously, it can be an arbritrary matrix. However a covariance matrix $C = X^T X$ has some interesting properties, namely it is always a positive semidefinite matrix ($v^T Cv \\geq 0$ for all non-zero $v \\in R^n$). Prove this statement.\n",
    "\n",
    "Hint: $v^T C v$ can be represented as a squared norm of some vector.\n",
    "\n",
    "\n",
    "\n",
    "### 4. Eigendecomposition (4 points)\n",
    "Consider the following matrix:\n",
    "$$\n",
    "M=\\begin{bmatrix}\n",
    "    1 & -1 & 0 \\\\\n",
    "    -1 & 2 & -1 \\\\\n",
    "    0 & -1 & 1\n",
    "  \\end{bmatrix}\n",
    "$$\n",
    "Is the matrix M:\n",
    "\n",
    "a. Symmetric? What does it imply for its eigendecomposition? (0.5 points)\n",
    "\n",
    "b. Singular? What does it imply for its eigendecomposition? (0.5 points)\n",
    "\n",
    "c. Find the eigendecomposition of M (3 points).\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### 5. Rayleigh-Ritz principle (3 points)\n",
    "On the lecture we saw that the exact solution for decoding matrix D in PCA problem is a matrix that contains the $l$ eigenvectors corresponding to the largest eigenvalues of $X^T X$, where X is a design matrix. But where does it come from?\n",
    "\n",
    "If you want to see full derivation of PCA, you can find it in Deep Learning Book, chapter 2.12. In this exercise we would like to focus only on Raleigh-Ritz principle, which allows to solve optimization problems of type $min_{||v||_2 = 1} v^T C v$ or $max_{||v||_2 = 1} v^T C v$ using eigendecomposition of C.\n",
    "\n",
    "So prove that $min_{||v||_2 = 1} v^T C v = \\lambda_{min}$, where $\\lambda_{min}$ is the smallest eigenvalue of C. For which vector $v$ this minimum is attained? (the proof for $max_{||v||_2 = 1} v^T C v = \\lambda_{max}$ is analogous)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow (9 points)\n",
    "\n",
    "### 1. A simple computational graph in Tensorflow (2 + 1 = 3 points)\n",
    "To get started with TensorFlow we ask you to specify a very simple computational graph of the function $f(x) = x^2 + 2x + 5$ and also taking its derivative.\n",
    "If you don't know how to proceed, please study the example given on the lecture and also the official TenorFlow starting guide: https://www.tensorflow.org/get_started/get_started.\n",
    "\n",
    "This exercise requires around 10 lines of code, it is simple enough. However, it is very important to understand the notion of computational graph. \n",
    "\n",
    "Actually, this simple example will be a backbone for all neural networks in the future. All what we will need is to specify first the forward pass (f), and then take derivatives with respect to parameters (x), in order to update them, so they minimize the loss function f. But we will come back to this in several weeks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  85.           83.20999146   81.44000244   79.68999481   77.96000671\n",
      "   76.25         74.55999756   72.89000702   71.23999786   69.61000061\n",
      "   68.           66.4099884    64.84000397   63.28999329   61.76000595\n",
      "   60.25         58.75999069   57.29000092   55.83999634   54.41000748\n",
      "   53.           51.61000061   50.24000549   48.88999939   47.55999756\n",
      "   46.25         44.9600029    43.69000244   42.43999481   41.20999908\n",
      "   40.           38.81000137   37.63999939   36.48999786   35.3599968\n",
      "   34.25         33.16000366   32.09000397   31.03999901   30.00999832\n",
      "   29.           28.01000214   27.04000282   26.08999825   25.15999985\n",
      "   24.25         23.36000061   22.49000168   21.63999748   20.80999756\n",
      "   20.           19.20999908   18.44000053   17.68999863   16.95999908\n",
      "   16.25         15.56000042   14.8900013    14.23999786   13.60999966\n",
      "   13.           12.4100008    11.84000015   11.29000092   10.75999928\n",
      "   10.25          9.76000023    9.28999901    8.84000015    8.40999985\n",
      "    8.            7.61000061    7.23999977    6.89000034    6.55999947\n",
      "    6.25          5.96000004    5.69000006    5.44000006    5.20999956\n",
      "    5.            4.80999994    4.63999987    4.48999977    4.36000013\n",
      "    4.25          4.15999985    4.09000015    4.03999996    4.01000023\n",
      "    4.            4.01000023    4.03999996    4.09000015    4.15999985\n",
      "    4.25          4.36000013    4.48999977    4.63999987    4.80999994\n",
      "    5.            5.21000004    5.44000006    5.69000006    5.96000004\n",
      "    6.25          6.55999994    6.88999987    7.23999977    7.60999966\n",
      "    8.            8.40999985    8.84000015    9.28999996    9.76000023\n",
      "   10.25         10.76000023   11.28999996   11.84000015   12.40999985\n",
      "   13.           13.6099987    14.23999977   14.88999939   15.56000042\n",
      "   16.25         16.95999908   17.69000053   18.43999863   19.21000099\n",
      "   20.           20.80999947   21.6400013    22.48999977   23.36000061\n",
      "   24.25         25.15999985   26.09000015   27.03999901   28.01000214\n",
      "   29.           30.00999832   31.0399971    32.09000397   33.15999985\n",
      "   34.25         35.36000061   36.48999786   37.63999939   38.81000137\n",
      "   40.           41.20999908   42.43999481   43.69000244   44.9600029\n",
      "   46.25         47.55999756   48.88999939   50.24000549   51.61000061\n",
      "   53.           54.40999985   55.83999634   57.29000092   58.76000214\n",
      "   60.25         61.75999832   63.28999329   64.84000397   66.41000366\n",
      "   68.           69.61000061   71.23999786   72.88999939   74.56000519\n",
      "   76.25         77.95999908   79.68999481   81.44000244   83.20999908\n",
      "   85.           86.81001282   88.63999939   90.48999786   92.35998535\n",
      "   94.25         96.16000366   98.08999634  100.04000092  102.00999451\n",
      "  104.          106.01000977  108.04000092  110.09000397  112.1599884\n",
      "  114.25        116.36000061  118.48999786  120.63999939  122.80999756\n",
      "  125.        ]\n",
      "[-18.         -17.79999924 -17.60000038 -17.39999962 -17.20000076 -17.\n",
      " -16.79999924 -16.60000038 -16.39999962 -16.20000076 -16.         -15.79999924\n",
      " -15.60000038 -15.39999962 -15.20000076 -15.         -14.79999924\n",
      " -14.60000038 -14.39999962 -14.20000076 -14.         -13.80000019\n",
      " -13.60000038 -13.39999962 -13.19999981 -13.         -12.80000019\n",
      " -12.60000038 -12.39999962 -12.19999981 -12.         -11.80000019\n",
      " -11.60000038 -11.39999962 -11.19999981 -11.         -10.80000019\n",
      " -10.60000038 -10.39999962 -10.19999981 -10.          -9.80000019\n",
      "  -9.60000038  -9.39999962  -9.19999981  -9.          -8.80000019\n",
      "  -8.60000038  -8.39999962  -8.19999981  -8.          -7.80000019\n",
      "  -7.60000038  -7.39999962  -7.19999981  -7.          -6.80000019\n",
      "  -6.60000038  -6.39999962  -6.19999981  -6.          -5.80000019\n",
      "  -5.5999999   -5.4000001   -5.19999981  -5.          -4.80000019\n",
      "  -4.5999999   -4.4000001   -4.19999981  -4.          -3.80000019\n",
      "  -3.5999999   -3.4000001   -3.19999981  -3.          -2.80000019\n",
      "  -2.5999999   -2.4000001   -2.19999981  -2.          -1.79999995\n",
      "  -1.5999999   -1.4000001   -1.20000005  -1.          -0.79999995\n",
      "  -0.5999999   -0.4000001   -0.20000005   0.           0.20000005\n",
      "   0.39999998   0.60000002   0.79999995   1.           1.20000005\n",
      "   1.39999998   1.60000002   1.79999995   2.           2.20000005\n",
      "   2.4000001    2.5999999    2.79999995   3.           3.20000005\n",
      "   3.4000001    3.5999999    3.79999995   4.           4.19999981\n",
      "   4.4000001    4.5999999    4.80000019   5.           5.19999981\n",
      "   5.4000001    5.5999999    5.80000019   6.           6.19999981\n",
      "   6.4000001    6.5999999    6.80000019   7.           7.19999981\n",
      "   7.4000001    7.5999999    7.80000019   8.           8.19999981\n",
      "   8.39999962   8.60000038   8.80000019   9.           9.19999981\n",
      "   9.39999962   9.60000038   9.80000019  10.          10.19999981\n",
      "  10.39999962  10.60000038  10.80000019  11.          11.19999981\n",
      "  11.39999962  11.60000038  11.80000019  12.          12.19999981\n",
      "  12.39999962  12.60000038  12.80000019  13.          13.19999981\n",
      "  13.39999962  13.60000038  13.80000019  14.          14.19999981\n",
      "  14.39999962  14.60000038  14.80000019  15.          15.19999981\n",
      "  15.39999962  15.60000038  15.80000019  16.          16.20000076\n",
      "  16.39999962  16.60000038  16.79999924  17.          17.20000076\n",
      "  17.39999962  17.60000038  17.79999924  18.          18.20000076\n",
      "  18.39999962  18.60000038  18.79999924  19.          19.20000076\n",
      "  19.39999962  19.60000038  19.79999924  20.          20.20000076\n",
      "  20.39999962  20.60000038  20.79999924  21.          21.20000076\n",
      "  21.39999962  21.60000038  21.79999924  22.        ]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x12649fc18>]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAD8CAYAAAB0IB+mAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzt3Xl4VOXZx/HvnR2SkBASCJAEAgQChNUAKqhYN1ARXIqg\nVtyKe9X2rZVqtW4tahd3W+pSrKBYF1RcUFHRKgIhbGEJCchOdkIWsud5/5gBhpBAQmbmzHJ/ritX\nZs6c5NycTH7cOec5zxFjDEoppXxfgNUFKKWUcg8NfKWU8hMa+Eop5Sc08JVSyk9o4CullJ/QwFdK\nKT+hga+UUn5CA18ppfyEBr5SSvmJIKsLcBQbG2t69+5tdRlKKeVVVq1aVWSMiTvReh4V+L179yYj\nI8PqMpRSyquIyI7WrKeHdJRSyk9o4CullJ/QwFdKKT+hga+UUn5CA18ppfyEBr5SSvkJDXyllPIT\nGvhKKWWx177/ia8257t8Oxr4SillobwD1fz50818lpXn8m1p4CullIVe/CaXxkbDnT9Lcfm2NPCV\nUsoie0qreGvFLqaOSiQxpqPLt9fqwBeRV0WkQESyHJY9JSKbRWSdiLwvItEOr80SkVwRyRaRC5xd\nuFJKebvnv8oF4I6z+7lle23p8P8NTGiy7AsgzRgzFNgCzAIQkUHANGCw/WteFJHAdlerlFI+Ymfx\nQf6bsYvpoxPpEd3BLdtsdeAbY74FSpos+9wYU29/+iOQYH88GXjLGFNjjPkJyAVGO6FepZTyCc99\nlUNAgHCbm7p7cO4x/BuAT+2PewK7HF7bbV+mlFJ+76eiSt5bvYdrxvSiW6cwt23XKYEvIvcD9cC8\nk/jamSKSISIZhYWFzihHKaU82rNLcggOFG4d39et22134IvIdcDFwNXGGGNfvAdIdFgtwb7sGMaY\nOcaYdGNMelzcCW/YopRSXi23oJyFa/Yw47TexEWGunXb7Qp8EZkA3AtcYow56PDSh8A0EQkVkWQg\nBVjRnm0ppZQvePrLHDoGB3LzWe7t7qENtzgUkTeB8UCsiOwGHsI2KicU+EJEAH40xtxijNkgIm8D\nG7Ed6rndGNPg7OKVUsqbbM4rY9G6fdx+dl9iwkPcvv1WB74xZnozi185zvqPA4+fTFFKKeWLnv4i\nh8jQIH55Rh9Ltq9X2iqllBus3VXKZxvyuGFcMtEd3d/dgwa+Ukq5xVOLs4kJD+GmM5Itq0EDXyml\nXOyH3CL+l1vEbeP7EhkWbFkdGvhKKeVCxhieWJxNj6gwrjm1l6W1aOArpZQLLd6Qz9pdpdx9bn/C\ngq2dUkwDXymlXKSh0fCXz7PpGxfOZSOtn11GA18ppVzkvczd5BZU8H/nDyAo0Pq4tb4CpZTyQTX1\nDTz9ZQ5DE6KYkBZvdTmABr5SSrnE/OU72VNaxb0XpGKficByGvhKKeVkFTX1PP9VLmP7dWFcSqzV\n5Rymga+UUk726v9+oriylt9ekGp1KUfRwFdKKScqrqhhzrfbmDA4nuGJ0Sf+AjfSwFdKKSd6dkkO\nVXUN/N8FA6wu5Rga+Eop5STbCiuYt3wn00Yl0q9rhNXlHEMDXymlnOSpxdmEBgVw97n9rS6lWRr4\nSinlBKt2lPBpVh43n9XX7bcubC0NfKWUaidjDI9/vImukaGWTn98Ihr4SinVTp9l5ZG5s5TfnN+f\njiGtvpGg22ngK6VUO9TWN/LEZ5vp3y2CK05JtLqc42p14IvIqyJSICJZDstiROQLEcmxf+7s8Nos\nEckVkWwRucDZhSullCeYv3wH24sPMmviQAIDPGMKhZa0pcP/NzChybL7gCXGmBRgif05IjIImAYM\ntn/NiyJi7UTQSinlZGXVdTyzJIfT+3Zh/IA4q8s5oVYHvjHmW6CkyeLJwFz747nAFIflbxljaowx\nPwG5wOh21qqUUh7lpW+2sv9gHb+/cKDHTJB2PO09ht/NGLPP/jgP6GZ/3BPY5bDebvsypZTyCXtK\nq3j1fz9x6YiepPWMsrqcVnHaSVtjjAFMW79ORGaKSIaIZBQWFjqrHKWUcqnZn24G8MgpFFrS3sDP\nF5HuAPbPBfblewDH09UJ9mXHMMbMMcakG2PS4+I8/xiYUkplbC/ho7V7ufmsvvSM7mB1Oa3W3sD/\nEJhhfzwD+MBh+TQRCRWRZCAFWNHObSmllOUaGw0Pf7SR+E5h3HJWH6vLaZNWXyEgIm8C44FYEdkN\nPATMBt4WkRuBHcBUAGPMBhF5G9gI1AO3G2ManFy7Ukq53buZu1m/5wB/v3KYR19k1ZxWV2uMmd7C\nS+e0sP7jwOMnU5RSSnmiipp6nlyczfDEaCYP875xKHqlrVJKtdKLX+dSWF7DQ5MGEeDhF1k1RwNf\nKaVaYVfJQV62D8MckdT5xF/ggTTwlVKqFf786SYCRfjdBM+6T21baOArpdQJ/LitmE/W53Hr+L7E\nR4VZXc5J08BXSqnjaGg0PPLRRnpEhTHzTO8ahtmUBr5SSh3HWyt3snFfGfddOJCwYO+eA1IDXyml\nWrC/spanFmczJjmGSUO7W11Ou2ngK6VUC55cnE15dT2PTE7zitkwT0QDXymlmrFudylvrdzJdaf3\nZkB8pNXlOIUGvlJKNdHYaPjDwixiI0K5+9wUq8txGg18pZRqYkHGLtbuPsD9Fw4kMizY6nKcRgNf\nKaUclB6s5cnPNjM6OYbJw3tYXY5TaeArpZSDpxZnU1ZdzyOTB/vEiVpHGvhKKWW3fvcB5q/YybWn\n9SI1vpPV5TidTwS+MYbvcgppbGzzHRaVUgqwn6j9IIsu4aHcc15/q8txCZ8I/P/lFvGLV1bw5sqd\nVpeilPJSCzJ2sWZXKbMmptLJh07UOvKJwB/XL5bT+nRh9qebKSivtrocpZSXKSyv4c+fbGJMcgyX\njfS+G5u0lk8Evojw+KVp1NQ38uiiTVaXo5TyMo99vJHqukYev3SIz52odeQTgQ/QJy6CO87ux0dr\n9/JNdoHV5SilvMS3Wwr5YM1ebh3fl35dI6wux6WcEvgico+IbBCRLBF5U0TCRCRGRL4QkRz7Z5ff\nIubms/rQNy6cBxZmUVWr90xXSh1fdV0DDyzMok9sOLeO72t1OS7X7sAXkZ7Ar4B0Y0waEAhMA+4D\nlhhjUoAl9ucuFRoUyJ8uHcLu/VU8syTH1ZtTSnm5Z5fksLPkII9dmub1Ux+3hrMO6QQBHUQkCOgI\n7AUmA3Ptr88FpjhpW8c1pk8XpqYn8PJ329icV+aOTSqlvFB2Xjlzvt3G5SMTOL1vrNXluEW7A98Y\nswf4C7AT2AccMMZ8DnQzxuyzr5YHdGvu60VkpohkiEhGYWFhe8sBYNbEgXTqEMys99br2Hyl1DEa\nGw2/f389kWFB3H/RQKvLcRtnHNLpjK2bTwZ6AOEico3jOsYYAzSbvMaYOcaYdGNMelxcXHvLAaBz\neAgPXDSQ1TtLmb9Cx+YrpY721spdrNqxn/svGkRMeIjV5biNMw7pnAv8ZIwpNMbUAe8BpwP5ItId\nwP7ZrUNnLh3Rk7H9uvDEZ5spKNOx+Uopm4LyamZ/uolT+8RwuQ+PuW+OMwJ/J3CqiHQU2wDWc4BN\nwIfADPs6M4APnLCtVhMRHpsyhNr6Rv7wQRa2PzKUUv7ujx9u8Isx981xxjH85cA7QCaw3v495wCz\ngfNEJAfbXwGz27uttkqODeee8/qzeEM+n6zPc/fmlVIe5pP1+/hkfR53nZtC3zjfHnPfHPGkzjc9\nPd1kZGQ49XvWNzRy2Us/sGd/FV/8+iy/Ol6nlDpif2Ut5/19KfFRYbx/21iCA33mulNEZJUxJv1E\n6/nOv7gFQYEBPHnFUMqq63j4ow1Wl6OUssgjizZSerCOJy8f5lNh3xZ+8a9Oje/E7Wf344M1e/ly\nY77V5Sil3Oyrzfm8v3oPt53dj0E9fG+e+9byi8AHuG18P1LjI7l/4XoOVNVZXY5Syk3Kquv4/XtZ\n9O9mm2/Ln/lN4IcE2Q7tFJbX8KePdUZNpfzFnz/ZREF5NU9eMYyQIL+JvGb51b9+aEI0M8/sy4KM\nXXyX45yrepVSnuv73CLeXLGLX57Rh+GJ0VaXYzm/CnyAu89NoU9cOPe9u57Kmnqry1FKuUhlTT33\nvbfu8PBs5YeBHxYcyJOXD2XvgSr+9Ike2lHKVz3x2WZ2lVTxxOVD/WImzNbwu8AHSO8dw03jkpm3\nfCdLt+ihHaV8zbdbCnl92Q5uHJfM6OQYq8vxGH4Z+AC/OX8AKV0juPedtZQerLW6HKWUkxw4WMe9\n76yjX9cIfnvBAKvL8Sh+G/hhwYH8/crhFFfU8uAHekGWUr7ioQ+zKKqo4e9Th+uhnCb8NvAB0npG\ncdc5KXy4di8frd1rdTlKqXb6eN0+Fq7Zy50/S2FIQpTV5Xgcvw58gFvH92V4YjQPLMwiX6dRVspr\nFZRV88DC9QxLiOK2s33//rQnw+8DPygwgL9NHUZNfQP3vrNOp1FWygsZY7jvvfUcrG3gb1cO99u5\nck5E9wrQJy6C3184kKVbCvUOWUp5oQUrd/HV5gJmTUz1y2mPW0sD3+6aMb04IyWWxxZtYntRpdXl\nKKVaaWfxQR5dtJGx/bpw7Wm9rS7Ho2ng2wUECE9eMZTgQOGuBWuoa2i0uiSl1AnUNzRy94LVBAQI\nT10xjIAA/7qDVVtp4DvoHtWBP182lLW7Svn7F1usLkcpdQLPLMkhc2cpf7p0CD2iO1hdjsfTwG/i\noqHdmTYqkZeWbuWH3CKry1FKtWDZ1mKe/zqXqekJTBrWw+pyvIJTAl9EokXkHRHZLCKbROQ0EYkR\nkS9EJMf+ubMztuUOD04aRJ/YcO5esIaSSr0KVylPs7+ylnsWrCG5Szh/vGSw1eV4DWd1+M8Anxlj\nUoFhwCbgPmCJMSYFWGJ/7hU6hgTx7PQRlB6s49531upQTaU8iDGGe99dR3FlDc9OH0HHkCCrS/Ia\n7Q58EYkCzgReATDG1BpjSoHJwFz7anOBKe3dljsN7hHFfRNT+XJTAa8v22F1OUopuzeW7+SLjfn8\nbkIqaT31atq2cEaHnwwUAq+JyGoReVlEwoFuxph99nXygG5O2JZbXT+2N2cPiOPxTzaxaV+Z1eUo\n5fey88p5bNFGzuofxw1jk60ux+s4I/CDgJHAS8aYEUAlTQ7fGNsxkWaPi4jITBHJEJGMwkLPmqpY\nRHjq58PoFBbMr95cTVVtg9UlKeW3qusauPPNTCLDgvnLz3UI5slwRuDvBnYbY5bbn7+D7T+AfBHp\nDmD/XNDcFxtj5hhj0o0x6XFxcU4ox7liI0L5+5XDyCmo4JFFOqumUlZ5ZNFGtuRX8Nepw4iLDLW6\nHK/U7sA3xuQBu0Tk0MTT5wAbgQ+BGfZlM4AP2rstq5yREset4/vy5opdvLtqt9XlKOV3Fq7ew/zl\nO7n5rD6c1d/zGkNv4azT23cC80QkBNgGXI/tP5O3ReRGYAcw1UnbssRvzutP5o793L9wPWk9oxgQ\nH2l1SUr5hZz8cma9t57RvWP47fl6Q5P2cMqwTGPMGvthmaHGmCnGmP3GmGJjzDnGmBRjzLnGmBJn\nbMsqQYEBPDd9BBGhwdw6bxUVegN0pVyusqaeW95YRXhoIM9dNYIgnQWzXXTvtUHXTmE8N30E24sq\n+d27OpWyUq5kjGHWe+v5qaiSZ6ePoFunMKtL8noa+G10Wt8u/N8FA/h43T4dn6+UC72xfCcfrt3L\nb84fwOl9Y60uxydo4J+EW87syzmpXXns442s3rnf6nKU8jlrd5Xy6EcbOXtAHLeepXevchYN/JMQ\nECD8deowunUK4475q9mv8+0o5TSlB2u5bV4mcZGh/G3qcB1v70Qa+CcpumMIL149ksLyGu5asIaG\nRj2er1R7NTQa7lmwhoLyal64eiSdw0OsLsmnaOC3w9CEaB6ePJhvtxTy5OLNVpejlNf76+fZfJ1d\nyIOTBjM8MdrqcnyOTjPXTtNHJ5G15wD/XLqNQd07MXl4T6tLUsorfbR2Ly9+s5XpoxO5ZkyS1eX4\nJO3wneChSYMZ3TuGe99ZR9aeA1aXo5TX2bD3AL99Zy3pvTrz8CVpiOhxe1fQwHeCkKAAXrxmJF3C\nQ5j5egZFFTVWl6SU1yiuqGHm66uI7hDCi9eMJCRIY8lVdM86SWxEKP/8RTrFlbXc9kYmtfV6E3Sl\nTqSuoZHb52dSWFHDnGtPoWukXlzlShr4TjQkIYonrxjKiu0lOrOmUq3w2KKN/LithNmXDWFogp6k\ndTU9aetkk4f3ZMPeMuZ8u41B3aO4Sk8+KdWst1fuYu6yHdw4LpnLRiZYXY5f0A7fBX43IZUz+8fx\n4AdZfJ9bZHU5SnmcH7YWcf/C9YzrF8usialWl+M3NPBdIDBAeP6qEfSJC+eWN1aRk19udUlKeYzc\nggpu+c8qenUJ54WrR+oMmG6ke9pFOoUF8+p1owgNCuT6f6+ksFxH7ihVXFHD9f9eQUhQAK9dN4qo\nDsFWl+RXNPBdKKFzR16ZkU5RRQ03vZ6h98RVfq26roFfvp5BQVkN/7o2ncSYjlaX5Hc08F1sWGI0\nT185gnW7S/n122to1Dl3lB9qbDT85u21ZO4s5ekrhzMiqbPVJfklDXw3mJAWz/0XDuTTrDye0Dl3\nlB966vNsPl6/j1kTU5k4pLvV5fgtHZbpJjeOS2Z7cSX/XLqNXjHhOlxT+Y23VuzkpW+2Mn10EjPP\n7GN1OX7NaR2+iASKyGoRWWR/HiMiX4hIjv2zX/8NJyL8cdJgxg+I44GF6/l8Q57VJSnlcks25XP/\nwizOSInlkcmDdY4ciznzkM5dwCaH5/cBS4wxKcAS+3O/FhQYwAtXjWRIQjR3vLma5duKrS5JKZdZ\ntaOE2+dnMqh7J1665hSCdfil5ZzyExCRBOAi4GWHxZOBufbHc4EpztiWtwsPDeK160aR2LkDN83N\nYMNenV1T+Z4t+eXc8O8Mukd14LXrRxERqkePPYGz/st9GrgXcJwxrJsxZp/9cR7QzUnb8nox4SH8\n58YxRIYFMePVleworrS6JKWcZk9pFde+Yhtr//oNo4mNCLW6JGXX7sAXkYuBAmPMqpbWMcYYoNnx\niCIyU0QyRCSjsLCwveV4jR7RHXj9xjE0NDZyzSvLKSirtrokpdqtpLKWX7yynMrael6/YbSOtfcw\nzujwxwKXiMh24C3gZyLyBpAvIt0B7J8LmvtiY8wcY0y6MSY9Li7OCeV4j35dI3jt+tEUV9Ry7asr\nOFBVZ3VJSp20suo6Zry6gt37q3j52nQGdu9kdUmqiXYHvjFmljEmwRjTG5gGfGWMuQb4EJhhX20G\n8EF7t+WLhidGM+cX6WwtrODGf6/kYG291SUp1WaVNfVc/9pKNueV8c9rTmFMny5Wl6Sa4crT5rOB\n80QkBzjX/lw1Y1xKLE9fOYLMnfu5aW4G1XU6BYPyHoemTFi9cz/PThvB2aldrS5JtcCpgW+M+cYY\nc7H9cbEx5hxjTIox5lxjTIkzt+VrLhranb/8fBjLthUz8z+rNPSVV6itb+TWN1axbFsxf506TK+i\n9XA6MNaDXDYygScuG8q3Wwq5fZ7eJlF5trqGRu56azVfZxfy+JQhXDpCb2Li6TTwPczUUYk8NiWN\nJZsLuPPNTOoaNPSV56lraORXb67m06w8Hrx4kE4V4iU08D3QNaf24qFJg1i8IZ+73lqtoa88Sm19\nI3fMz+TTrDz+cPEgbhiXbHVJqpX08jcPdf3YZBoNPLpoI7X1mbxw9QhCgwKtLkv5udr6Rm6fn8kX\nG/P546RBXDdWw96baIfvwW4cl8yjkwfz5aZ8btYTucpiNfUN3DZvFV9szOeRyYM17L2QBr6H+8Vp\nvZl92RCWbinkxrk6Tl9Zo6q2gZv/s4ovNxXw6JQ0rj2tt9UlqZOgge8Fpo1O4i9XDGPZ1mKue20l\n5dV6Ra5yn7LqOq59dTlLtxQy+7Ih/OLUXlaXpE6SBr6XuPyUBJ6ZNoLMHfuZ/q8fKarQm6Ir1yuq\nqGH6nB9Zs6uU56ePZNpoHY3jzTTwvcikYT3414x0cgsquOKlH9hVctDqkpQP21NaxdR/LGNrYQX/\nujadi4bqRVXeTgPfy5w9oCvzbhpDSWUtl7/0A5vzyqwuSfmg3IJypv5jGYXlNfznxjGMH6DTJfgC\nDXwvdEqvGP57y+mIwNR/LCNju85aoZxnxU8lXP7SMmrqG3lz5qmM6h1jdUnKSTTwvdSA+EjeueV0\nukSEctXLy1m0bq/VJSkf8PG6fVzz8nK6RITw/m2nk9YzyuqSlBNp4HuxxJiOvHvr6QztGcUd81fz\n0jdbsd1rRqm2Mcbw8nfbuH1+JkMTonj3ltP15iU+SAPfy8WEh/DGTWOYNKwHT3y2md+/v16nYlBt\nUt/QyMMfbeSxjzdx4ZB43rhpDJ3DQ6wuS7mATq3gA8KCA3nmyuEkxXTgha+3snt/FS9cPZJOYcFW\nl6Y83IGDddzxZibf5RRx07hkfn/hQAICxOqylItoh+8jAgKE316QypOXD2XZ1mKmPP89uQUVVpel\nPNjWwgqmvPg9P24r5snLh/LAxYM07H2cBr6PmToqkXk3jeFAVR2XvvA9SzblW12S8kDfbilkygvf\nU1ZVx/xfnsrUUYlWl6TcQAPfB43p04UP7xxHr9iO3PR6Bs9/laMncxVgOzn7j6Vbue61FSR07sgH\nd4zVYZd+RAPfR/WM7sB/bz6dS4b14C+fb+HWNzIp0zl4/NqBqjpm/mcVsz/dzMS07rxzy2kkdNaR\nOB7hYAkc2O3yzbT7pK2IJAKvA90AA8wxxjwjIjHAAqA3sB2YaozZ397tqdbrEBLI01cOJ61HFLM/\n28yk5/7HC1eN1LHVfmjj3jJunbeKPfurePDiQVw/tjcierze7RrqoWQr5K2H/A2Qn2X7XLYHhvwc\nLn/ZpZuX9v6pLyLdge7GmEwRiQRWAVOA64ASY8xsEbkP6GyM+d3xvld6errJyMhoVz2qeSu3l3Dn\n/NWUVNby4KRBXD0mSX/h/YAxhv+u2s0fFmYR3TGYF64aSboewnGPgyVHAj0vy/a4cDPUV9teDwiC\n2AHQbTDEp0HiqZA05qQ2JSKrjDHpJ1zP2cd2ReQD4Hn7x3hjzD77fwrfGGMGHO9rNfBdq7iihnve\nXsu3WwqZNKwHf7o0jUgduumzDlTV8cDCLD5au5fT+nThuatGEBsRanVZvqehHopz7eHuEPDlDle/\nd4y1hXo3+0d8GsT2hyDn/DwsCXwR6Q18C6QBO40x0fblAuw/9LzJ18wEZgIkJSWdsmPHDqfVo47V\n2Gh4aelW/vp5Nj2iO/C3qcMZnawdn69Zub2Eu99aQ15ZNb8+rz+3nNWXQB1y2X6HuvY8e7Dnr4eC\nzdBgn678UNcen2br3A8FfGQ3l5bl9sAXkQhgKfC4MeY9ESl1DHgR2W+M6Xy876Edvvus2lHCPQvW\nsmv/QW45qy/3nNufkCA9h+/t6hsaee6rXJ77KoeEzh15ZtpwRiQd99dONadp134o4B279vA4e6AP\nhvghts+xAyDI/VcptzbwnXKlrYgEA+8C84wx79kX54tId4dDOgXO2JZyjlN6xfDJXWfw6Ecbeemb\nrXy7pZCnrxxOSrdIq0tTJyk7r5zfvrOWdbsPcNnInjx8yWA9ZNcaB0uanETNatK1B0PcAEg+4+iA\nj/C+KaOdcdJWgLnYTtDe7bD8KaDY4aRtjDHm3uN9L+3wrbF4Qx6z3ltPRU09d52Twswz+xAcqN2+\nt6hraOSfS7fy7JJcIsKCeHRymt6spDkN9VCcYz/G7hDw5fuOrBPe9chJ1EOHY2L7W9K1t4XbDumI\nyDjgO2A9cGjWrt8Dy4G3gSRgB7ZhmceduF0D3zqF5TU89GEWn6zPY2D3Tjx5+VCGJOjwTU+3aV8Z\nv31nLVl7yrh4aHcevmQwXfTE7LFde956KMw+tms/3LHbw90Lu3awcJROe2jgW2/xhjz+sDCLoooa\nfnlGH+4+tz8dQgKtLks1UVlTz7NLcnjlfz8R3TGYRyenMXGIH3b1re3aD59EPXSs3fO79rZw6zF8\n5TsuGBzPqX26MPvTTfzz220sWrePBy4ayIS0eB237wGMMSzekMfDH21k34FqrkxP5HcTU4nxh+mM\nK4ubDH1soWtPPuvoUTJe2rW7gnb4qkXLtxXz0Icb2JxXzth+XfjjpMF6UtdC24sqefijDXydXUhq\nfCSPX5rGKb18cEhtQ51thMyhi5UOBbxj1x7R7ehhj/Fp0CXFp7r2ttBDOsop6hsambd8J3/9PJuD\ntQ3MOL03d/6sH9Ed/fMXywollbU8uySHN37cQWhQAL8+fwAzTutFkC+cWK8sto1lb3o1akOt7fWA\nYIhLPXZce0SctXV7GA185VTFFTX85fNs3lq5i4jQIG4d35frT0/W4/suVF3XwNwftvP817lU1tRz\n5agk7jkvha6RYVaX1nYNdVCUc+RipUMBX5F3ZJ3muvbY/hCoQ0tPRANfucTmvDKe+iybJZsL6NYp\nlLvO6c/U9ATf6DY9RG19I+9m7ub5r3LZU1rFz1K7ct/EVPp7y+G0yqJjr0YtzNau3YU08JVLrfip\nhNmfbiJzZylJMR25dXxfLhvZk9Ag7fhPVtOgH5YQxb0TUhnbL9bq0pp3uGtvcjXqMV1706tRtWt3\nNg185XLGGJZsKuC5r3JYu/sA3aPCmHlmH6aNStJDPW1QVdvAO5m7+cc3W21BnxjN3eemML5/nOeM\njKosOvZqVMeuPTDEYVy74wgZ7drdQQNfuY0xhu9yinj+61xW/FRCl/AQrh6TxNWn9qJbJy883uwm\n+WXVvL5sO/OW76T0YJ1nBH1DHRRtOXZce4XDrTIj4pu5GjVFu3YLaeArS6zcXsI/l25lyeYCAkWY\nkBbP9WN7MzKps+d0qxYyxpC5s5R5y3fw0dq91Dcazh/UjZvO6EN6Lzfvo6Zde559hEyj/c5oh7v2\nIUcHfLiHHmLyY3rhlbLEqN4xjOodw87ig7y+bDsLMnaxaN0+UuMjueKUBKaM6OmXc7KXVNbyXuZu\nFqzcRU6P9fuAAAAMs0lEQVRBBR1DArl6TC+uH9ubXl3CXbvx1nbt8WnQ72cOV6Nq1+5rtMNXLlVZ\nU8/7q/fw34xdrN19gKAAYfyAOC4fmcDZqV0JC/bdY/0Ha+v5enMhi9bt5ctN+dQ1GIYnRjNtVCIX\nD+tBRKgL+q2KwmNvxHFM1556ZNjjoWPt2rV7NT2kozxOTn4572Tu5v3MPRSU19AxJJDxA+K4YHA8\nZ6d2pZMPTOVbUVPPd1sKWbR+H19tKqCqroHYiFAmDevOlaMSSY3v5JwN1dfa5pBpejWqY9ce2b2Z\nq1H7adfugzTwlceqb2hk2bZiPsvK4/ON+RSW1xAcKJzapwvj+sUytl8sg7p3IsAL7tBkjGFzXjlL\ntxSyNLuQjB0l1DUYYiNCmJAWz0VDejA6OaZ9d5uqKGxyNeqG5rv2Q8MeD42S0a7db2jgK6/Q2GhY\nvWs/n2Xl8U12ITkFFQBEdwzmtD5dSO8dw7CEKAb3iPKIoZ419Q1k7Skjc8d+Vu3YT8aO/RRV2Cbv\nSo2P5KwBcYzv35VRvTu3/WK0+tojx9odA77S4d5B2rWrZmjgK6+UX1bND1uL+D63mGVbi9lTWgVA\nYIDQv1skQ3p2ol/XCPrERtC3awSJnTu45Crf+oZG8sqqyS2oIDuvnOz8crbkl7Mlv4LaetttH3p1\n6cjIpM6c1rcLZ/WPa9sQ1IqCJlej2se1H7drT4PwLk7/tyrvp4GvfEJBWTVrdx9g3e5S1uwqZePe\nMooraw+/HhwoxEeF0S0yjG6dwoiLDCUuMpSI0CA6hgQSbv8cFBBAozE0GIMxhvoGQ0VNPWVVdRyo\nqqesuo7iihr2lFaxt7SavLJqGhqP/G7Edwqjf3wkA+MjGdmrMyOTOhMX2YrRRoe79iZXox7TtTe5\nGlW7dtUGGvjKZx04WMfWogq2FlSwtbCSfQeqKCirIb+8moKyGipq6tv8PcNDAomJCKFHVAd6Rneg\nZ+cOdI/qQL+uEfTvFtG62UErCppcjbqhSdceCl1Tj70aVbt21U46Dl/5rKiOwYxMsnXZzamua6Cy\npp6DtQ0crG2goqaeRmMIEBARAkUIDBAiQoOI6hBMZFhQ2w4L1ddCUfax49orC4+sE9nDFugp5x0J\n+C79IFB/5ZR1XP7uE5EJwDNAIPCyMWa2q7ep/FtYcCBhwYE4pW8uzz92XHtRNjTa/4o41LWnXHDk\natSug7VrVx7JpYEvIoHAC8B5wG5gpYh8aIzZ6MrtKtVmh7r2puPam3bt8WnQ/3zt2pVXcvU7dTSQ\na4zZBiAibwGTAQ18ZZ3y/GPHtR/TtQ+0de2OV6N29MHbCSq/4urA7wnscni+Gxjj4m0qZVNfYxsh\nc7yuvVNPW6D3v+DIKJmYvtq1K59k+btaRGYCMwGSkpIsrkZ5JWPs49pb0bX3v+DoUTLatSs/4urA\n3wMkOjxPsC87zBgzB5gDtmGZLq5Hebv6GttQR8cbceRlwcGiI+t06mkL9P4XHJnSV7t2pVwe+CuB\nFBFJxhb004CrXLxN5QuMsU0E1vRq1KItR7r2oDBb1z5gwpEpfbVrV6pFLg18Y0y9iNwBLMY2LPNV\nY8wGV25TeaHDXXvW0WPbm+vaB0y0B/sQiOmjXbtSbeDy3xZjzCfAJ67ejvICh7r2pidRtWtXyi20\nPVKuUV9jm8L38ElU+8fB4iPrdEqwhbl27Uq5hf5mqfYxBsrzjp3St2gLmAbbOoe79guPTOnbdZB2\n7Uq5mQa+ar26aoerUR0CvmnXHp8GqRc6XI3aFwKsn8teKX+nga+OdbhrbzKl71Fde4cjXXu8w7H2\nDs1PaKaUsp4Gvr9rTdcelWgL89SLHK5G7aNdu1JeRgPfX7Sla0+9yOFq1EHatSvlIzTwfVFd9ZER\nMo4BX1VyZJ2oRFugp17kcDWqdu1K+TINfG9mDJTvO/ZGHEU5R3ft3QbBwIuPHtfeIdra2pVSbqeB\n7y0Od+1NrkZtrmsfOMlhXHuydu1KKUAD3/Mc6tqPuRpVu3alVPto4FuprhoKNx17NWrV/iPrRCXZ\nwly7dqVUO2ngu4MxULb32KtRi3OPdO3BHW0jZAZecvTVqNq1K6WcRAPf2eqqbMfaHaf0ba5rj0+D\nQZccGf6oXbtSysU08E/W4a69ybj24hwwjbZ1gjvauvSBlxx9NWpYlLW1K6X8kgZ+a9RVQcEmh459\nw7Fde3SSrVMfNPnI1aide2vXrpTyGBr4joyBsj3Hjmsvzj22ax80+eirUbVrV0p5OP8N/MNde9bR\no2SqS4+sE51kGxUzaMqRq1E7J0NAgHV1K6XUSfL9wG9V1x5u69IHT9GuXSnls3wr8NvStQ++1H4S\nVbt2pZR/aFfgi8hTwCSgFtgKXG+MKbW/Ngu4EWgAfmWMWdzOWlu2dw2898sWuvZLj5xE7TpQu3al\nlN9qb4f/BTDLGFMvIk8As4DficggYBowGOgBfCki/Y05dJWRk4XHQZcU7dqVUuo42hX4xpjPHZ7+\nCFxhfzwZeMsYUwP8JCK5wGhgWXu216KonjB9vku+tVJK+QpntsA3AJ/aH/cEdjm8ttu+TCmllEVO\n2OGLyJdAfDMv3W+M+cC+zv1APTCvrQWIyExgJkBSUlJbv1wppVQrnTDwjTHnHu91EbkOuBg4xxhj\n7Iv3AIkOqyXYlzX3/ecAcwDS09NNc+sopZRqv3Yd0hGRCcC9wCXGmIMOL30ITBORUBFJBlKAFe3Z\nllJKqfZp7yid54FQ4AsRAfjRGHOLMWaDiLwNbMR2qOd2l43QUUop1SrtHaXT7zivPQ483p7vr5RS\nynl0oLpSSvkJDXyllPITcmRgjfVEpBDY0Y5vEQsUOakcZ9K62kbrahutq218sa5expi4E63kUYHf\nXiKSYYxJt7qOprSuttG62kbraht/rksP6SillJ/QwFdKKT/ha4E/x+oCWqB1tY3W1TZaV9v4bV0+\ndQxfKaVUy3ytw1dKKdUCnwh8EZkgItkikisi91lYR6KIfC0iG0Vkg4jcZV/+RxHZIyJr7B8XWlDb\ndhFZb99+hn1ZjIh8ISI59s+d3VzTAId9skZEykTkbiv2l4i8KiIFIpLlsKzF/SMis+zvt2wRucDN\ndT0lIptFZJ2IvC8i0fblvUWkymG//cPNdbX4c7N4fy1wqGm7iKyxL3fn/mopG9z7HjPGePUHEIjt\n9op9gBBgLTDIolq6AyPtjyOBLcAg4I/A/1m8n7YDsU2WPQncZ398H/CExT/HPKCXFfsLOBMYCWSd\naP/Yf6Zrsc0jlWx//wW6sa7zgSD74ycc6urtuJ4F+6vZn5vV+6vJ638FHrRgf7WUDW59j/lChz8a\nyDXGbDPG1AJvYbvjltsZY/YZYzLtj8uBTXj2jV8mA3Ptj+cCUyys5RxgqzGmPRfenTRjzLdASZPF\nLe2fw3d0M8b8BBy6o5tb6jLGfG6Mqbc//RHb9ONu1cL+aoml++sQsc3wOBV40xXbPp7jZINb32O+\nEPgeeXctEekNjACW2xfdaf8T/FV3HzqxM9juLbzKftMZgG7GmH32x3lANwvqOmQaR/8iWr2/oOX9\n40nvOcc7zQEk2w9PLBWRMyyop7mfm6fsrzOAfGNMjsMyt++vJtng1veYLwS+xxGRCOBd4G5jTBnw\nErZDTsOBfdj+rHS3ccaY4cBE4HYROdPxRWP7O9KSIVsiEgJcAvzXvsgT9tdRrNw/LZFj7zS3D0iy\n/5x/DcwXkU5uLMnjfm5NTOfopsLt+6uZbDjMHe8xXwj8Vt9dyx1EJBjbD3SeMeY9AGNMvjGmwRjT\nCPwLF/05ezzGmD32zwXA+/Ya8kWku73u7kCBu+uymwhkGmPy7TVavr/sWto/lr/n5Mid5q62BwX2\nP/+L7Y9XYTvu299dNR3n5+YJ+ysIuAxYcGiZu/dXc9mAm99jvhD4K4EUEUm2d4rTsN1xy+3sxwhf\nATYZY/7msLy7w2qXAllNv9bFdYWLSOShx9hO+mVh208z7KvNAD5wZ10Ojuq8rN5fDlraP5be0U1a\nuNOciMSJSKD9cR97XdvcWFdLPzdPuAPeucBmY8zuQwvcub9aygbc/R5zxxlqV38AF2I7670V283V\nrapjHLY/ydYBa+wfFwL/Adbbl38IdHdzXX2wnfFfC2w4tI+ALsASIAf4EoixYJ+FA8VAlMMyt+8v\nbP/h7APqsB0vvfF4+we43/5+ywYmurmuXGzHdw+9x/5hX/dy+893DZAJTHJzXS3+3KzcX/bl/wZu\nabKuO/dXS9ng1veYXmmrlFJ+whcO6SillGoFDXyllPITGvhKKeUnNPCVUspPaOArpZSf0MBXSik/\noYGvlFJ+QgNfKaX8xP8D00WVhzBP1m4AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x125ce89e8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "### Define a simple comp. graph (note that all its components are special TensorFlow objects)\n",
    "# Specify an input x to the comp. graph\n",
    "\n",
    "x = tf.placeholder(tf.float32)\n",
    "\n",
    "# Specify f(x) = x^2 + 2x + 5 \n",
    "f = x ** 2 + 2 * x + 5\n",
    "\n",
    "# Take derivative of f with respect to x. That will be very important in the future, since you will only\n",
    "# need to specify forward pass of your neural network, and all derivatives will be determined automatically by \n",
    "# TensorFlow.\n",
    "\n",
    "#d = tf.gradients(f, x)\n",
    "d = 2 * x + 2\n",
    "\n",
    "### Execute the comp. graph\n",
    "# Create a TensorFlow session\n",
    "sess = tf.Session()\n",
    "\n",
    "# Evaluate the function value and its derivative for values from -10 to 10 with step 0.1\n",
    "f_values = sess.run(f, {x : [v/10 for v in range(-100,101)]})\n",
    "d_values = sess.run(d, {x : [v/10 for v in range(-100,101)]})\n",
    "print(f_values)\n",
    "print(d_values)\n",
    "\n",
    "# Evaluate values of f and its derivative based on a particular input x\n",
    "f_100 = sess.run(f, {x: 100})\n",
    "d_100 = sess.run(f, {x: 100})\n",
    "\n",
    "# Plot the function f (use matplotlib library)\n",
    "plt.plot(f_values)\n",
    "\n",
    "# Plot the derivative of the function f\n",
    "plt.plot(d_values)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tensorflow Questions:\n",
    "1.) In the exercise above x should be a TF placeholder, but f is a TF tensor. What is the difference between a placeholder and other tensors in computational graph? And how TF Variable differs from tensors? (0.5 point)\n",
    "\n",
    "A TF placeholder is a dummy node, i.e. an entry point to the computational graph for data to be provided later. Normal TF tensors just consist of a set of primitive values shaped into an array. TF variables are in-memory buffers containing tensors: Unlike tensors, they only hold values after being explicitly initialized.\n",
    "\n",
    "\n",
    "2.) What does tf.Session().run(...) accept as an argument and what does it return? What is the fundamental difference between these 2 types of objects? (0.5 point)\n",
    "\n",
    "tf.Session().run(...) accepts TF nodes and returns a value of the same shape: A single value if the argument was a single graph element, a list in case of a list, a dictionary with the same keys in case of a dictionary. The fundamental difference between before and after tf.Session().run(...) then is evaluation. The method is one step in an overall TF computation that runs the graph fragments necessary to excute every operation and to evaluate every tensor\n",
    "in the argument it receives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. A pure Python implementation (1 point)\n",
    "Please do the same, but in pure Python, calculating the expression for the derivative by hand. There is no need to use NumPy library, since we deal with 1-dimensional x (0.5 point). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Evaluate the function value and its derivative for values from -10 to 10 with step 0.1\n",
    "\n",
    "# Define f\n",
    "def f(x):\n",
    "    return x ** 2 + 2 * x + 5\n",
    "\n",
    "# Define f_derivative_val\n",
    "def f_derivative_val(x):\n",
    "    return 2 * x + 2 \n",
    "\n",
    "# Plot the function f\n",
    "f_vals = [f(v/10) for v in range(-100,101)]\n",
    "plt.plot(f_vals)\n",
    "\n",
    "# Plot the derivative of the function f\n",
    "f_derivative_vals = [f_derivative_val(v/10) for v in range(-100,101)]\n",
    "plt.plot(f_derivative_vals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. A more complicated computational graph (5 points)\n",
    "Okay, you could easily take the derivative of $f(x) = x^2 + 2x + 5$ by hand. But what if you need to take the derivative of the function $f(x) = \\sigma(-6 \\sigma (x + 5 x^{-2} + 3 x^{-3}) + 2 \\sigma (-x + 4 x^{-2}))$ (where $\\sigma = \\frac{1}{1 + exp(-x)}$) with respect to x?\n",
    "It resembles a neural network much more than the previous example.\n",
    "Please, implement it in TensorFlow (0.5 point) and in pure Python (0.5 point) in the same way as before (but using new cells with the code below). \n",
    "\n",
    "Note that in pure Python implementation you will need to somehow overcome the numerical errors for small values of x (roughly from -0.1 to 0.1). You can just skip these values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For pure Python implementation you will need to derive the derivative by hand (3 points). Please write down your derivation for the new $f(x)$ (you are encouraged to write LaTeX for them):\n",
    "\n",
    "The following variables are used to simplify the derivation and make everything more readable:\n",
    "\n",
    "$a = (-6 \\sigma (x + 5 x^{-2} + 3 x^{-3})$\n",
    "\n",
    "$b = 2 \\sigma (-x + 4 x^{-2}))$\n",
    "\n",
    "For the quotient we get after applying the quotient rule right at the beginning of the derivation: $q = \\left(\\mathrm{e}^{-x}+1\\right)^2 $\n",
    "\n",
    "$\\frac{df}{dx}$ = $ \\frac{(\\frac{df}{dx}[a+b] * (\\mathrm{e}^{-x}+1)) - ((a+b) * \\frac{df}{dx} [\\mathrm{e}^{-x}+1])} {q}$ \n",
    "\n",
    "$=  \\frac{(\\frac{(-6 \\frac{df}{dx}[x + 5x^{-2} + 3x^{-3}] * (\\mathrm{e}^{-x}+1) - (x + 5x^{-2} + 3x^{-3}) * \\frac{df}{dx} [\\mathrm{e}^{-x}+1])}{q}) + (\\frac{(2 \\frac{df}{dx}[-x + 4x^{-2}] * (\\mathrm{e}^{-x}+1) - (-x + 4x^{-2}) * \\frac{df}{dx} [\\mathrm{e}^{-x}+1])}{q}) * (\\mathrm{e}^{-x}+1) - (\\mathrm{e}^{-x} * \\frac{df}{dx}[-x] + 0) * (a+b)} {q} $\n",
    "\n",
    "$= \\frac{\\mathrm{e}^{-x} * (a+b) + (\\mathrm{e}^{-x} +1) * (-6*(\\frac{(\\frac{df}{dx}[x] + 5* \\frac{df}{dx}[x^{-2}] + 3* \\frac{df}{dx}[x^{-3}])*(\\mathrm{e}^{-x}+1)-(\\frac{df}{dx}[\\mathrm{e}^{-x}] + \\frac{df}{dx}[1]) * (x + 5x^{-2} + 3 x^{-3}))}{q}) + 2* (\\frac{((4 * \\frac{df}{dx}[x^{-2}] - \\frac{df}{dx}[x])*(\\mathrm{e}^{-x}+1)-(\\frac{df}{dx}[\\mathrm{e}^{-x}] + \\frac{df}{dx}[\\frac{df}{dx}[1]) * (4x^{-2} - x))}{q}))}{q} $\n",
    "\n",
    "$= \\frac{\\mathrm{e}^{-x} * (a+b) + (\\mathrm{e}^{-x} +1) * (-6*(\\frac{(1 + 5* (-2) * x^{-3} + 3* (-3) * x^{-4})*(\\mathrm{e}^{-x} * (\\frac{df}{dx}[x] + 0) * (x + 5x^{-2} + 3 x^{-3}))}{q}) + 2 * (\\frac{(4 * (-2) * x^{-3} -1) * (\\mathrm{e}^{-x}+1)-(\\mathrm{e}^{-x} * \\frac{df}{dx}[-x] + 0) * (4x^{-2} - x))}{q}))}{q} $\n",
    "\n",
    "$= \\frac{\\mathrm{e}^{-x} * (a+b) + (\\mathrm{e}^{-x} +1) * (-6*(\\frac{(-10x^{-3} - 9 x^{-4} + 1)*(\\mathrm{e}^{-x} +1) + (x + 5x^{-2} + 3 x^{-3}) * \\mathrm{e}^{-x}}{q}) + 2 * \\frac{((-8x^{-3} -1) * (\\mathrm{e}^{-x} +1) + (4x^{-2} - x) * \\mathrm{e}^{-x})}{q} }{q} $\n",
    "\n",
    "The derived expression for $\\frac{df}{dx}$ should produce exactly the same values (up to the float type precision) as TensorFlow derivative.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.88075793  0.88075376  0.88074923  0.88074416  0.8807385   0.88073242\n",
      "  0.88072556  0.88071799  0.88070959  0.88070023  0.88069004  0.88067865\n",
      "  0.88066614  0.88065225  0.88063687  0.88061988  0.88060111  0.88058025\n",
      "  0.88055718  0.88053173  0.88050354  0.88047224  0.88043773  0.88039947\n",
      "  0.88035715  0.88031018  0.88025826  0.88020086  0.88013715  0.88006657\n",
      "  0.87998837  0.87990177  0.8798058   0.87969953  0.87958175  0.87945098\n",
      "  0.87930614  0.87914544  0.87896711  0.87876934  0.87854981  0.87830603\n",
      "  0.87803531  0.87773454  0.87740028  0.87702858  0.87661511  0.87615502\n",
      "  0.87564272  0.87507188  0.87443548  0.87372553  0.87293285  0.87204707\n",
      "  0.87105638  0.86994714  0.86870384  0.8673085   0.86574048  0.86397588\n",
      "  0.86198652  0.85973984  0.85719752  0.85431451  0.85103685  0.84730101\n",
      "  0.84303045  0.83813316  0.8324976   0.82598811  0.81843871  0.80964524\n",
      "  0.79935586  0.7872588   0.77296865  0.75600982  0.73580045  0.71164125\n",
      "  0.68271679  0.64812762  0.60698068  0.55857599  0.50272423  0.44018456\n",
      "  0.37309027  0.30507207  0.24073523  0.18447146  0.13916895  0.10564832\n",
      "  0.08319273  0.07101485  0.07160974  0.11051992  0.46852183  0.87370014\n",
      "  0.88079697  0.88079703  0.01798621  0.01798621  0.01798621  0.0179862\n",
      "  0.01798525  0.01796597  0.01783576  0.01738581  0.0163972   0.0148455\n",
      "  0.0129663   0.01109155  0.00945514  0.0081388   0.00712303  0.00634973\n",
      "  0.00575836  0.00529924  0.00493541  0.00464059  0.00439649  0.00419039\n",
      "  0.00401347  0.00385951  0.00372406  0.0036039   0.00349663  0.0034004\n",
      "  0.00331378  0.00323561  0.00316492  0.00310091  0.00304291  0.0029903\n",
      "  0.00294256  0.00289923  0.00285989  0.00282417  0.00279174  0.00276228\n",
      "  0.00273553  0.00271124  0.00268917  0.00266914  0.00265095  0.00263443\n",
      "  0.00261943  0.00260581  0.00259345  0.00258223  0.00257205  0.0025628\n",
      "  0.00255441  0.00254679  0.00253988  0.00253361  0.00252792  0.00252276\n",
      "  0.00251808  0.00251383  0.00250998  0.00250649  0.00250332  0.00250044\n",
      "  0.00249784  0.00249547  0.00249333  0.00249139  0.00248963  0.00248803\n",
      "  0.00248658  0.00248527  0.00248408  0.00248301  0.00248203  0.00248114\n",
      "  0.00248034  0.00247961  0.00247895  0.00247836  0.00247782  0.00247733\n",
      "  0.00247688  0.00247648  0.00247612  0.00247579  0.00247549  0.00247522\n",
      "  0.00247497  0.00247475  0.00247455  0.00247437  0.0024742   0.00247405\n",
      "  0.00247392  0.0024738   0.00247368  0.00247358  0.00247349]\n",
      "[array([ -3.93431001e-05,  -4.35109214e-05,  -4.81214374e-05,\n",
      "        -5.32217891e-05,  -5.88642797e-05,  -6.51069204e-05,\n",
      "        -7.20136522e-05,  -7.96554523e-05,  -8.81111628e-05,\n",
      "        -9.74678187e-05,  -1.07822016e-04,  -1.19280790e-04,\n",
      "        -1.31962443e-04,  -1.45998973e-04,  -1.61535543e-04,\n",
      "        -1.78734452e-04,  -1.97774585e-04,  -2.18854446e-04,\n",
      "        -2.42195703e-04,  -2.68042291e-04,  -2.96666840e-04,\n",
      "        -3.28370865e-04,  -3.63490224e-04,  -4.02397505e-04,\n",
      "        -4.45507030e-04,  -4.93279658e-04,  -5.46227966e-04,\n",
      "        -6.04922476e-04,  -6.69999339e-04,  -7.42165023e-04,\n",
      "        -8.22209986e-04,  -9.11013922e-04,  -1.00956007e-03,\n",
      "        -1.11894694e-03,  -1.24040048e-03,  -1.37529522e-03,\n",
      "        -1.52516831e-03,  -1.69174524e-03,  -1.87696423e-03,\n",
      "        -2.08299630e-03,  -2.31229188e-03,  -2.56760907e-03,\n",
      "        -2.85206176e-03,  -3.16917012e-03,  -3.52291437e-03,\n",
      "        -3.91782308e-03,  -4.35903436e-03,  -4.85241041e-03,\n",
      "        -5.40464371e-03,  -6.02339860e-03,  -6.71748491e-03,\n",
      "        -7.49704940e-03,  -8.37380998e-03,  -9.36137419e-03,\n",
      "        -1.04755489e-02,  -1.17348330e-02,  -1.31609216e-02,\n",
      "        -1.47793498e-02,  -1.66203976e-02,  -1.87199991e-02,\n",
      "        -2.11211443e-02,  -2.38754638e-02,  -2.70453095e-02,\n",
      "        -3.07063721e-02,  -3.49510387e-02,  -3.98926847e-02,\n",
      "        -4.56712581e-02,  -5.24603017e-02,  -6.04760088e-02,\n",
      "        -6.99888244e-02,  -8.13379213e-02,  -9.49491262e-02,\n",
      "        -1.11356109e-01,  -1.31223515e-01,  -1.55369431e-01,\n",
      "        -1.84776217e-01,  -2.20576108e-01,  -2.63971210e-01,\n",
      "        -3.16031158e-01,  -3.77270877e-01,  -4.46887970e-01,\n",
      "        -5.21597624e-01,  -5.94249845e-01,  -6.53019905e-01,\n",
      "        -6.82728350e-01,  -6.69746876e-01,  -6.09431505e-01,\n",
      "        -5.10926604e-01,  -3.93853784e-01,  -2.77994573e-01,\n",
      "        -1.72852397e-01,  -6.82017207e-02,   1.06532931e-01,\n",
      "         9.40047979e-01,   7.57074261e+00,   4.53249156e-01,\n",
      "         1.34355478e-05,   2.29185113e-18,   0.00000000e+00,\n",
      "        -7.07275251e-19,  -9.22174975e-11,  -4.26025650e-07,\n",
      "        -3.65872875e-05,  -4.91885585e-04,  -2.47886055e-03,\n",
      "        -6.92665065e-03,  -1.29003264e-02,  -1.77069511e-02,\n",
      "        -1.92865618e-02,  -1.78115256e-02,  -1.47915464e-02,\n",
      "        -1.15788523e-02,  -8.83916859e-03,  -6.72971969e-03,\n",
      "        -5.18062897e-03,  -4.06301394e-03,  -3.25695984e-03,\n",
      "        -2.66942754e-03,  -2.23353691e-03,  -1.90287968e-03,\n",
      "        -1.64579542e-03,  -1.44082191e-03,  -1.27341074e-03,\n",
      "        -1.13366498e-03,  -1.01479865e-03,  -9.12100135e-04,\n",
      "        -8.22255039e-04,  -7.42881908e-04,  -6.72232884e-04,\n",
      "        -6.08995499e-04,  -5.52154845e-04,  -5.00906084e-04,\n",
      "        -4.54595662e-04,  -4.12679394e-04,  -3.74696043e-04,\n",
      "        -3.40247236e-04,  -3.08986055e-04,  -2.80605367e-04,\n",
      "        -2.54832208e-04,  -2.31422542e-04,  -2.10157072e-04,\n",
      "        -1.90837673e-04,  -1.73285924e-04,  -1.57339615e-04,\n",
      "        -1.42852150e-04,  -1.29690510e-04,  -1.17733631e-04,\n",
      "        -1.06871790e-04,  -9.70052934e-05,  -8.80433072e-05,\n",
      "        -7.99035624e-05,  -7.25111240e-05,  -6.57978526e-05,\n",
      "        -5.97018006e-05,  -5.41665540e-05,  -4.91410174e-05,\n",
      "        -4.45785554e-05,  -4.04368475e-05,  -3.66773747e-05,\n",
      "        -3.32651362e-05,  -3.01683285e-05,  -2.73579826e-05,\n",
      "        -2.48078177e-05,  -2.24939467e-05,  -2.03946020e-05,\n",
      "        -1.84900491e-05,  -1.67623766e-05,  -1.51952390e-05,\n",
      "        -1.37738252e-05,  -1.24846947e-05,  -1.13156002e-05,\n",
      "        -1.02554550e-05,  -9.29416183e-06,  -8.42255577e-06,\n",
      "        -7.63232310e-06,  -6.91590776e-06,  -6.26646352e-06,\n",
      "        -5.67775851e-06,  -5.14414432e-06,  -4.66048687e-06,\n",
      "        -4.22214089e-06,  -3.82487542e-06,  -3.46486036e-06,\n",
      "        -3.13862256e-06,  -2.84300245e-06,  -2.57514125e-06,\n",
      "        -2.33244009e-06,  -2.11254724e-06,  -1.91332947e-06,\n",
      "        -1.73284764e-06,  -1.56934709e-06,  -1.42123281e-06,\n",
      "        -1.28706472e-06,  -1.16553451e-06,  -1.05545246e-06,\n",
      "        -9.55746827e-07,  -8.65438835e-07], dtype=float32)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x129186f60>]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW4AAAD9CAYAAACcJ53WAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAGo5JREFUeJzt3Xt0nHd95/H3dy66S7ZkydjxPcYEQuLEiZoEElgaSknS\nQspt1yzpSXcLZrO75drTDYezC+0fdAuF3Zy2S2ogu2UJDksIbbZLFyiEppuSEFtxYidOsJPYsWXH\nkXyRLNuy5vLdP+YZeWYiWWNZc9FvPq9zdGbm0TOjr58Zf/TTd37Pb8zdERGR+SNW6wJEROT8KLhF\nROYZBbeIyDyj4BYRmWcU3CIi84yCW0RknikruM3sk2b2tJntNLMtZtZS6cJERGRqMwa3mS0DPgb0\nu/tlQBzYWOnCRERkauW2ShJAq5klgDbgYOVKEhGRc5kxuN19EPhT4CXgEDDi7j+qdGEiIjK1xEw7\nmFk3cCuwBjgOfNfMbnP3b5XstwnYBNDe3n7161//+gqUKyISpm3btg27e185+84Y3MCvAS+6+xCA\nmT0AvBkoCm533wxsBujv7/etW7eeV9EiIo3MzPaVu285Pe6XgOvMrM3MDHg7sGu2xYmIyIUpp8f9\nGHA/MADsiO6zucJ1iYjINMppleDunwM+V+FaRESkDDpzUkRknlFwi4jMMwpuEZF5RsEtIjLPKLgl\nOMN/O8yZwTO1LkOkYhTcEpyn3/s0B7+m5XQkXApuCYq74ynHJ7zWpYhUjIJbwpLNXXhWwS3hUnBL\nUDwTBXa2tnWIVJKCW4KSH2lPBrhIgBTcEpZMdKkRtwRMwS1BmRxxq8ctAVNwS1DyLRK1SiRkCm4J\nS7bkUiRACm4JyuSIW60SCZiCW8KSH2lnzrmXyLym4JagaMQtjUDBLUGZDGz1uCVgCm4JS9Qi0awS\nCZmCW4KiedzSCBTcEhStVSKNQMEtYcmvDqhWiQRMwS1B0YhbGoGCW8Ki9bilASi4JShaq0QagYJb\ngqJ53NIIFNwSlvw8brVKJGAKbgnKZGBrrRIJmIJbgqK1SqQRKLglLFqPWxqAgluColkl0ggU3BIW\nzeOWBqDglqDozElpBApuCcrk6oBqlUjAFNwSlvw0QI24JWAKbgmK1uOWRqDglqBoVok0AgW3hEXz\nuKUBKLglKDpzUhpBWcFtZgvN7H4ze9bMdpnZmypdmMis5EfaWqtEApYoc7+7gP/r7u83syagrYI1\nicyaRtzSCGYMbjNbALwV+B0Ad58AJipblsjsaD1uaQTltErWAEPAfzezJ8zs62bWXuG6RGYnvx63\nZpVIwMoJ7gRwFfBVd98AnATuLN3JzDaZ2VYz2zo0NDTHZYqUR/O4pRGUE9wHgAPu/lh0+35yQV7E\n3Te7e7+79/f19c1ljSJl01ol0ghmDG53fxnYb2aXRJveDjxT0apEZiu/OqBaJRKwcmeV/B5wbzSj\n5AXgX1WuJJHZ04hbGkFZwe3u24H+CtcicuG0Hrc0AJ05KUHRWiXSCBTcEhTN45ZGoOCWsOTncatV\nIgFTcEtQJgNba5VIwBTcEhStVSKNQMEtYdF63NIAFNwSFM0qkUag4JawaB63NAAFtwRFZ05KI1Bw\nS1AmVwdUq0QCpuCWsOSnAWrELQFTcEtQCnvb7hp1S5gU3BKUwhaJ2iUSKgW3hCU7zXWRgCi4JShF\nI25NCZRAKbglLIWjbK1XIoFScEtQNOKWRqDglqAUhbV63BIoBbeEpaA9olklEioFtwSlaB63WiUS\nKAW3BKVolK1WiQRKwS1hKQhrtUokVApuCYpG3NIIFNwSlsIRt3rcEigFtwRFa5VII1BwS1A0j1sa\ngYJbwlI4j1utEgmUgluCUhTWWqtEAqXglqBorRJpBApuCYvW45YGoOCWoGhWiTQCBbeERfO4pQEo\nuCUoOnNSGoGCW4JStDqgWiUSKAW3hKVwCqBG3BIoBbcERetxSyNQcEtQNKtEGoGCW8KSBeIF10UC\npOCWoHjGiSVzL2u1SiRUZQe3mcXN7Akz+9tKFiRyQbJgCctd11olEqjzGXF/HNhVqUJE5oJnHEvm\nglsjbglVWcFtZsuB3wC+XtlyRC6MZ88Gt3rcEqpyR9z/FfgD9F9B6l2GsyNuzSqRQM0Y3Gb2m8Ar\n7r5thv02mdlWM9s6NDQ0ZwWKnA/P6s1JCV85I+7rgXeb2V7gPuBGM/tW6U7uvtnd+929v6+vb47L\nFClPYY9bfx9KqGYMbnf/jLsvd/fVwEbgp+5+W8UrE5mNglklapVIqDSPW4KiEbc0gsT57OzuPwN+\nVpFKROZCFk0HlOBpxC1BKTpzUq0SCZSCW4KiedzSCBTcEpaMWiUSPgW3BMWzrrVKJHgKbgmK1iqR\nRqDglrBkmXxzUj1uCZWCW4JSNOLWrBIJlIJbwqJ53NIAFNwSFJ05KY1AwS1BKZxVolaJhErBLWHJ\n6M1JCZ+CW4JSeOaketwSKgW3BMPdi9+cVKtEAqXglnBEOT155qRaJRIoBbcEIz/C1ohbQqfglnBE\nI2y9OSmhU3BLMF414tabkxIoBbcEIx/UapVI6BTcEo5oGVeLGxhqlUiwFNwSjMkRd9wgplaJhEvB\nLcGYbI3EwGKmD1KQYCm4JRxRa8TihsVNI24JloJbglE44iaGetwSLAW3hKNwxB0zzSqRYCm4JRiT\n87hjBnG9OSnhUnBLMCaDOh6Ft1olEigFt4QjP487Fk0HVKtEAqXglmAUzuO2uEbcEi4FtwSjdB63\netwSKgW3hKNgVolaJRIyBbcEo2jErVaJBEzBLeEoHXGrVSKBUnBLMArncWutEgmZgluCUTSPW2uV\nSMAU3BKOknnc6nFLqBTcEoyiedxaq0QCpuCWYBStDqi1SiRgCm4JR8nqgGqVSKgU3BKM0vW41SqR\nUM0Y3Ga2wsweMrNnzOxpM/t4NQoTOW8ln4CjEbeEKlHGPmng0+4+YGadwDYz+7G7P1Ph2kTOS+k8\nbvW4JVQzjrjd/ZC7D0TXTwC7gGWVLkzkfBXO41arREJ2Xj1uM1sNbAAeq0QxIhekYB63WiUSsrKD\n28w6gO8Bn3D30Sm+v8nMtprZ1qGhobmsUaQshfO4tVaJhKys4DazJLnQvtfdH5hqH3ff7O797t7f\n19c3lzWKlKV0PW6tVSKhKmdWiQHfAHa5+1cqX5LILJXMKtGIW0JVzoj7euC3gRvNbHv0dUuF6xI5\nb6XzuNXjllDNOB3Q3f8fYFWoReTClJw5mc0ouSVMOnNSglE4j1trlUjIFNwSjKL1uLVWiQRMwS3h\nKFmPWyfgSKgU3BKMovW4dQKOBEzBLcEoncetHreESsEt4Sj9lHe1SiRQCm4JRtGIW60SCZiCW8JR\nOuJWq0QCpeCWYJSux621SiRUCm4JRtE8bq1VIgFTcEs4SuZxq8ctoVJwSzCK5nHHTLNKJFgKbglG\n0eqAWqtEAqbglnCUrA6oVomESsEtwShdj1utEgmVglvCUfIJOBpxS6gU3BKMyXncZlqrRIKm4JZg\neNYhHt1Qq0QCpuCWcGSiOdxorRIJm4JbguFZzwU2aK0SCdqMHxZcTf+4e4hM9myfMi9/rWATFm0t\n3lZ6ZYb9Cn7OVN+3oo9IPtd+519r0SNb8X5F26Z4nHgMYmYkYjFiMYou42bE40bc7Oz3rLjGUGVO\nZoi15sYi+RNwHn3hCOOpDAtak2xY2V3jCkXmRl0F96ZvbuN0SisDVUI8lgvzRNxoTsRoScbPXhZe\njy47WxJ0tSTpak3Q2ZKkqyVBV2uS3vZmXtPVzKKOZuKx+vplkBpOkexLAvDLoRMkUllu3/zo5Pcf\n+v23saa3vVblicyZugruLZuuI+uOF/2Fm7tRuC1/tWhbdKPwrvnvO1PcebrHOefP81dtY6b7+qt2\nm/JxzuffnHUnky34cieddbLZkkt30pnc9zPZLJkspDNZzqSzjKcyr7ocPZ2avH5iPM3o6RQTmakb\nxTGDvs5mlnS1sGpRO6t727m4t503XtTFxX0dNQn11HCKZG8uuF8+cYaVDt/63Wt58chJ/uNf7+TQ\nyGkFtwShroL7yhULa12ClBhPZRgdTzF6Os3I6RRHxs5w+MQZXhkd5/DoOIdGxnli/zH+91MHJ3/R\ntDXFueyiBbxp7SLedkkf65cvrEqQp4ZTtKxqAeDo6QlWu3HDul4WdTQBcPxUquI1iFRDXQW31J+W\nZJyWZJzFnefebzyVYd+RU+wcHGHH4AhP7D/On/10N3f9ZDfdbUluuXwpt123ijcs7apYranhFJ1X\n5Qo9cjqFeQx3p7stF9zHTk1U7GeLVJOCW+ZESzLOJUs6uWRJJ++7ejkAx05O8PDuIX767Cvcv+0A\n9z72Elev6ubDN6zhpsuWzOkbpu4+2SoZOZViLJUGmsBhYVuufaIRt4RCwS0V093exK1XLuPWK5dx\n/NQE9287wLce3ccd9w5wxfIF/NGtl3HFHLXHMicz+Bkn2Ztkz9AJPPqd4BmnJRmnNRnn6EmNuCUM\nmsctVbGwrYkPv+VifvLpt/Gl96/n5dFx3vPfHuELP9jFRPrCz5RJDedG08neJHteGSObH8xHD93T\n3qRWiQRDwS1VFY8ZH+hfwY8/9c/YeM1KNj/8Ah/4y59z8PjpC3rc9JE0cDa4Y9GJOPmTcBa2JdUq\nkWAouKUmulqSfOE9l/PVD13F86+M8Vt/8Qg7B0dm/XilI+7uaCZJfr2S7jaNuCUcCm6pqZsvX8r3\n7ngziZixcfOjbNt3dFaPUxTcQ2P0dOWCO98qWdiW5Jh63BIIBbfU3CVLOnng315Pb0cTt9/z+KzC\nOx/c6S7jwLHT9HY1A+Cp3Ig71+NWq0TCoOCWurBkQQv3bXrTrMM7NZyCGDx/Zhx3WLy2A4Dx/eNA\n7s3R0fHU5Fo4IvOZglvqRml4D7x0rOz7poZTJBclee7wCQDWXJ1bUOr0ntybnt1tSdxh5LRG3TL/\nKbilruTDe1FHE7ff8wueOnC8rPvlT77ZdWiU9qY4qzbk5oef3p0Pbp09KeFQcEvdWbKghW9/5DoW\ntCb57W/8oqzZJvkR966XT3DJkk6aFiRpWtI0Gdz5syf1BqWEQMEtdWnZwla2fOQ62pvi3PaNx9i2\n79xtk9RwikRvgmcPjU6uh9K6rnWyVdLTnh9xq1Ui85+CW+rWip42tmzKjbw/+LVH+esnBqfdNzWc\nItUVY3Q8zevzwf3aVrVKJEgKbqlrqxa188Adb+aK5Qv4xHe287EtT/DyyHjRPvkFpo435SZtv2FJ\nboXA1nWtTLw8QXosXbDQlIJb5r+ygtvMbjKz58xsj5ndWemiRAot6mhmy0eu41PveB0/2HGIt37p\nIT7zwA5+/nzuY8nSI2k87ew4NQbk5oVDbsQNuZklHc0JEjHj6Em1SmT+m3F1QDOLA38BvAM4ADxu\nZg+6+zOVLk4kLxGP8bG3r+M9G5bxZz/dzfefOMCWX7xEPGb86t5mbiPOj9IjfPiGNXS25EbXreui\n4N59ms4rO+lub9KIW4JQzrKu1wB73P0FADO7D7gVmPPgHtsxNnmKcvEn+r56m539hN3Z71dw3ab8\nFODK7zdjjVPtFwdL2NmvuDXEhwFDru/9xfdfwefe9Ub+cfcwOwdHWPmjYcZ70vzhH/8K/Rcvmty3\ndW0uuEceGaHtdW287niC3Y8M80cvDWBmWAyIGWa5Dxc2I3e8E0amw/C2GGZGLGbEzEjGcx/QnIif\nvZ6MG4l4jETMSGIkHOIZg4xjaYc0kAHDMQc89zxadB6QFXzhuSs2eY6QFbwMHIvWqp18pqPHq7m5\neOn5hf9DbE4KuTDJ5hjXvOOiiv+ccoJ7GbC/4PYB4NpKFDNw3QDZUxe+xGdDKg3zwlAvvJ004m1x\nYm0x4q3RZXQ71hoj0ZkgsShBsidJoid3mVyUJNmXJNGdqJtfEO3NCW66bAk39vXwT9sPseLTK1hb\nENoAic4EzSuaGbxrkMG7BvkIkOsOjpb1MzLmnG6BUy3OqRY41eQk0tCUhuaU0ZSKLtOQSEOsAsHh\nJZdS3450OJyoj+Aui5ltAjYBrFy5claPcemWS3OruRV9sm50MeUn585+v6J9p/p+Nfabocbp9vOs\n4+nz/yID2Yks2dNZMqcypI6kyB7IXc+eyl1mxjJn/+opEe+I07K6Jfe1poW2S9ro2NBBxxUdxNvj\nU9+pwg5/+zBkYMntS6b8/vofrufUrlO54+bRaytbcD368uzZ29mJLOmRNOljadLHo69jadKjaaw5\n9wuOthjWEoPWGN5ikDSyccgmgLjhCYPol6UnmPIvKLdooBkNuSevR2VNXi+5DwX7nR2u10a9/EKp\nlzoWt1Rnvkc5wT0IrCi4vTzaVsTdNwObAfr7+2d1HHvf3Tubu8kc8qznQutomtSRFKmjKdJH0kwc\nnmB83zjje8cZ3zfO8YePkxnN5O5k0L6+nZ539tDzzh4WXL+AWHPlX8CpYyle+s8v0XV9F+2XTv3p\n7e1vaKf9DfpkdwlLOcH9OLDOzNaQC+yNwL+saFVSMxYzkt1Jkt3JyR7xVNydM4NnGBsY48TACUYe\nHuHAfznA/i/uJ74gzuKNi1l2xzI6ruioWK0vfvZFUsMp1v/d+or9DJF6NGNwu3vazP498EMgDtzj\n7k9XvDKpa2ZGy/IWWpa3TP6llD6R5vhDxxn67hCHv3mYQ395iJ6beljzhTV0bpjhY+LP0+hjoxy8\n+yDLPrZs8pPdRRqF+Ry8m1uqv7/ft27dOuePK/NH6liKg3cfZP+X95M+mmbppqWs/dJaEp0X/rZK\nNp1l4FcGmBia4Jpd18zJY4rUmpltc/f+cvbVmZNSEcnuJKs+s4pr91zL8k8u59DmQzx++eOc2Hbi\ngh978M8HGds+xrq71im0pSEpuKWikguTvPbLr2XDIxvAYeD6AQ5vOTzrx5sYnmDv5/bSc1MPve/V\nm9nSmBTcUhUL3rSAq7deTde1Xez60C4G755+wahz2feH+8iczLD2K2vrZk65SLUpuKVqmvqaWP/D\n9Sz6jUXsvmM3B79+8Lzuf+q5Uxy8+yAXbbpIU/ykoSm4pariLXHe+MAb6bm5h19+9JcMPzhc9n2f\n/w/PE2uNsfrzqytXoMg8oOCWqoslY1z6vy6l86pOnvkXzzDyTzN/ws3xfzjOkb85wso7V9K0uKkK\nVYrULwW31ESiI8Hl/+dympc3s+NdOzi56+S0+2bTWfZ8cg/Ny5tZ/snlVaxSpD4puKVmmhbnet6W\nNJ666SnOHDwz5X77/3Q/Y0+MsfYra4m31mZNFJF6ouCWmmq9uJX1P1hP+miaJ3/9yVeF99iTY+z9\n/F5639fL4g8srlGVIvVFwS0113lVJ5c9eBln9p1h4M0DHHvoGO7OyM9H2H7jdpLdSdb9+bpalylS\nN3TamdSF7l/t5sqfXcmOd+3gyRufJL4gTmYkQ8uaFq748RU0L2mudYkidUPBLXWj8+pOrn3+Wg5/\n8zAntp6g/fJ2Fn9wMU19mkUiUkjBLXUl3hrnoo9eBB+tdSUi9Us9bhGReUbBLSIyzyi4RUTmGQW3\niMg8o+AWEZlnFNwiIvOMgltEZJ5RcIuIzDMV+ZR3MxsC9s3y7r1A+avrV1e91lavdYFqmy3VNjv1\nWls5da1y975yHqwiwX0hzGxruR9RX231Wlu91gWqbbZU2+zUa21zXZdaJSIi84yCW0RknqnH4N5c\n6wLOoV5rq9e6QLXNlmqbnXqtbU7rqrset4iInFs9jrhFROQc6ia4zewmM3vOzPaY2Z01rmWFmT1k\nZs+Y2dNm9vFo++fNbNDMtkdft9Sovr1mtiOqYWu0rcfMfmxmu6PL7hrUdUnBsdluZqNm9olaHTcz\nu8fMXjGznQXbpj1OZvaZ6PX3nJm9s8p1fcnMnjWzp8zs+2a2MNq+2sxOFxy7uytV1zlqm/b5q9Yx\nO0dt3ymoa6+ZbY+2V/u4TZcZlXm9uXvNv4A48DxwMdAEPAlcWsN6lgJXRdc7gV8ClwKfB36/Do7X\nXqC3ZNsXgTuj63cCf1IHz+nLwKpaHTfgrcBVwM6ZjlP0/D4JNANrotdjvIp1/TqQiK7/SUFdqwv3\nq9Exm/L5q+Yxm662ku9/GfhPNTpu02VGRV5v9TLivgbY4+4vuPsEcB9wa62KcfdD7j4QXT8B7AKW\n1aqeMt0K/FV0/a+A36phLQBvB55399meiHXB3P1h4GjJ5umO063Afe5+xt1fBPaQe11WpS53/5G7\np6ObjwLLK/GzZzLNMZtO1Y7ZTLWZmQH/HNhSqZ9/LufIjIq83uoluJcB+wtuH6BOgtLMVgMbgMei\nTb8X/Tl7Ty3aEREH/t7MtpnZpmjba9z9UHT9ZeA1tSlt0kaK/xPVw3GD6Y9TPb0G/zXwdwW310R/\n7v+Dmb2lRjVN9fzV0zF7C3DY3XcXbKvJcSvJjIq83uoluOuSmXUA3wM+4e6jwFfJtXOuBA6R+9Os\nFm5w9yuBm4F/Z2ZvLfym5/4Wq9l0ITNrAt4NfDfaVC/HrUitj9NUzOyzQBq4N9p0CFgZPd+fAr5t\nZl1VLqsun78SH6R4oFCT4zZFZkyay9dbvQT3ILCi4PbyaFvNmFmS3BNwr7s/AODuh9094+5Z4GtU\n8M/Cc3H3wejyFeD7UR2HzWxpVPtS4JVa1Ba5GRhw98NQP8ctMt1xqvlr0Mx+B/hN4EPRf3KiP6WP\nRNe3keuFvq6adZ3j+av5MQMwswTwXuA7+W21OG5TZQYVer3VS3A/DqwzszXRaG0j8GCtion6Zd8A\ndrn7Vwq2Ly3Y7T3AztL7VqG2djPrzF8n96bWTnLH6/Zot9uBv6l2bQWKRj/1cNwKTHecHgQ2mlmz\nma0B1gG/qFZRZnYT8AfAu939VMH2PjOLR9cvjup6oVp1RT93uuevpseswK8Bz7r7gfyGah+36TKD\nSr3eqvWuaxnvyt5C7p3Y54HP1riWG8j9SfMUsD36ugX4n8COaPuDwNIa1HYxuXejnwSezh8rYBHw\nE2A38PdAT42OXTtwBFhQsK0mx43cL49DQIpcD/F3z3WcgM9Gr7/ngJurXNcecj3P/Ovt7mjf90XP\n83ZgAHhXDY7ZtM9ftY7ZdLVF2/8H8G9K9q32cZsuMyryetOZkyIi80y9tEpERKRMCm4RkXlGwS0i\nMs8ouEVE5hkFt4jIPKPgFhGZZxTcIiLzjIJbRGSe+f90ygpJif5lvQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1291b3d68>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "# Please paste the code from your above TensorFlow implementation, \n",
    "# but specify the new f(x) and produce the corresponding plots\n",
    "z = tf.placeholder(tf.float32)\n",
    "\n",
    "# Specify f(x)\n",
    "#First define sigmoid helper function:\n",
    "def sigmoid (x):\n",
    "    return 1/(1 + tf.exp(-x))\n",
    "\n",
    "f = sigmoid(-6* sigmoid(z + 5 * z ** -2 + 3 * z ** -3) + 2 * sigmoid(-z + 4 * z ** -2))\n",
    "\n",
    "# Take derivative of f with respect to x. That will be very important in the future, since you will only\n",
    "# need to specify forward pass of your neural network, and all derivatives will be determined automatically by \n",
    "# TensorFlow.\n",
    "\n",
    "d = tf.gradients(f,z)\n",
    "\n",
    "### Execute the comp. graph\n",
    "# Create a TensorFlow session\n",
    "sess = tf.Session()\n",
    "\n",
    "# Evaluate the function value and its derivative for values from -10 to 10 with step 0.1\n",
    "f_values = sess.run(f, feed_dict = {z : np.concatenate((np.arange(-10, -0.2, 0.1),np.arange(0.2, 10.01, 0.1)))})\n",
    "d_values = sess.run(d, feed_dict = {z: np.concatenate((np.arange(-10, -0.2, 0.1),np.arange(0.2, 10.01, 0.1)))})\n",
    "#print(np.append(np.arange(-10, -0.1, 0.1), np.arange(0.2, 10.01, 0.1)))\n",
    "print(f_values)\n",
    "print(d_values)\n",
    "\n",
    "# Plot the function f (use matplotlib library)\n",
    "plt.plot(f_values)\n",
    "# Plot the derivative of the function f\n",
    "plt.plot(d_values[0], color='m')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Please paste the code from your above pure Python implementation, \n",
    "# but specify the new f(x) and its derivative that you calculated manually and produce the corresponding plots\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Questions (1 point)\n",
    "1.) What is the main difference in the program structure between TensorFlow and plain Python? (0.5 point)\n",
    "\n",
    "...\n",
    "\n",
    "2.) Does TensorFlow provide numerical or automatic differentiation? What are advantages of this way of differentiation? (0.5 point)\n",
    "It provides numerical differentiation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission instructions\n",
    "You should provide a single Jupyter notebook as a solution. The naming should include the assignment number and matriculation IDs of all team members in the following format:\n",
    "**assignment-1_matriculation1_matriculation_2_matriculation3.ipynb** (in case of 3 team members). \n",
    "Make sure to keep the order matriculation1_matriculation_2_matriculation3 the same for all assignments.\n",
    "\n",
    "Please, submit your solution to your tutor (with **[NNIA][assignment-2]** in email subject):\n",
    "1. Maksym Andriushchenko s8mmandr@stud.uni-saarland.de\n",
    "2. Marius Mosbach s9msmosb@stud.uni-saarland.de\n",
    "3. Rajarshi Biswas rbisw17@gmail.com\n",
    "4. Marimuthu Kalimuthu s8makali@stud.uni-saarland.de\n",
    "\n",
    "**If you are in a team, please submit only 1 solution to only 1 tutor.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
